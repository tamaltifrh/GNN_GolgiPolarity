{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Import libraries ....\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "def graphLoad(filename):\n",
    "    # Load data from .mat files ....\n",
    "    curr_dir = os.getcwd(); \n",
    "    data_dir = os.path.join(curr_dir,'Matlab')\n",
    "    gMat_fname_test = os.path.join(data_dir, filename)\n",
    "    gMat_test = sio.loadmat(gMat_fname_test,struct_as_record=False)\n",
    "    print('Processing File: ', gMat_fname_test) # shows the content of Matlab files\n",
    "    \n",
    "    # Create graph from Matlab structure ....\n",
    "    # Needs to substract 1 from initial node index\n",
    "    gn = gMat_test['gn']\n",
    "    sourceNode = np.array(gn[0,0].source)-1; targetNode = np.array(gn[0,0].target)-1\n",
    "    nE = sourceNode.shape[0]; ST = np.zeros((nE,2), dtype=float); \n",
    "    ST = np.column_stack((sourceNode,targetNode))\n",
    "    # print(sourceNode.shape,targetNode.shape, ST.shape)\n",
    "\n",
    "    # Read edges from source and target nodes ....\n",
    "    collective_edges = pd.DataFrame(ST, columns=['source', 'target'])\n",
    "\n",
    "    # Read other network properties ....\n",
    "    # ..................................\n",
    "    nodeXY = np.array(gn[0,0].nodeXY)      #XY-coodinate of the node\n",
    "    nodeLabel = np.array(gn[0,0].nodeLabel)-1 # Node labels; Matlab starts with 1, Python with 0\n",
    "    nodeClass = np.array(gn[0,0].nodeClass) # Node classes\n",
    "    nF = gMat_test['num_features']\n",
    "    nclass = np.array(gn[0,0].numClasses).item()\n",
    "    \n",
    "    # Read pre-normalized features ....\n",
    "    feature_matrix = torch.tensor(gMat_test['feature_matrix_n'],dtype=torch.float)\n",
    "\n",
    "    # Build graph .... Needs to create torch tensors\n",
    "    ST = torch.tensor(ST, dtype=torch.long) \n",
    "    nodeClass = torch.tensor(nodeClass, dtype=torch.long) \n",
    "    graphData = Data(x=feature_matrix[:,:], edge_index=ST.t().contiguous(),y=nodeClass.t())\n",
    "\n",
    "    # Show graph properties\n",
    "    print('====================================================')\n",
    "    print('Graph properties::')\n",
    "    print('Nodes: ',graphData.num_nodes)\n",
    "    print('Edges: ',graphData.num_edges)\n",
    "    print('Features: ',graphData.num_node_features)\n",
    "    print('Classes: ', nclass)\n",
    "    print('====================================================')\n",
    "    print('Edges: Source Node to Target Node')\n",
    "    print(collective_edges) # print edges\n",
    "    print('====================================================')\n",
    "    \n",
    "    return graphData, nclass\n",
    "\n",
    "# Use graphLoad function to load train and test data .... \n",
    "train_graphData, nclass = graphLoad('train_data_10F.mat')\n",
    "test_graphData, test_nclass = graphLoad('test_data_10F.mat')\n",
    "\n",
    "# Load graph data to the device ....\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_data = train_graphData.to(device)\n",
    "test_data = test_graphData.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Declare the Net class ....\n",
    "# Performance Experience: ChebConv ~> GraphConv >> SAGEConv >> GCNConv\n",
    "# Batch Normalization improves the convergence\n",
    "# GraphConv works even for smaller number of hidden layers e.g. 6 and medium dropout = 0.25;\n",
    "# It works better with 'mean' or 'max' aggregation than 'add', and without a bias\n",
    "# Depth = 2 works better than depth = 3 \n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GCNConv, GraphConv, ChebConv\n",
    "from torch.nn import BatchNorm1d\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,nfeat, nhid, nclass, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # self.conv1 = GraphConv(nfeat, nhid, aggr='add',bias=False)\n",
    "        # self.conv1 = SAGEConv(nfeat, nhid, normalize=True, bias=False)\n",
    "        # self.conv1 = GCNConv(nfeat, nhid,bias=False)\n",
    "        self.conv1 = ChebConv(nfeat, nhid, K = 2, bias=False)\n",
    "        \n",
    "        self.bn1 = BatchNorm1d(nhid)\n",
    "        \n",
    "        # self.conv2 = GraphConv(nhid, nclass, aggr='add',bias=False)\n",
    "        # self.conv2 = SAGEConv(nhid, nclass, normalize=True, bias=False)\n",
    "        # self.conv2 = GCNConv(nhid, nclass, bias=False)\n",
    "        self.conv2 = ChebConv(nhid, nclass, K = 2, bias=False)\n",
    "        \n",
    "        self.dropout=dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "# Defining the hyperparameters ....\n",
    "# Less number of hidden units => Slower convergence, higher loss, but BETTER accuracy\n",
    "# nhid = 2*Features and dropout = 0.5 is good choice\n",
    "nhid = 20; dropout = 0.5\n",
    "numEpoch = 600\n",
    "\n",
    "# Create the model with defined hyperparameters\n",
    "model = Net(train_graphData.num_node_features, nhid, nclass, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.train()\n",
    "\n",
    "hist = {\"loss\":[],\"accuracy\":[]}\n",
    "for epoch in range(numEpoch):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data)\n",
    "    targetLabel = train_data.y.t().squeeze_()\n",
    "    loss = F.nll_loss(out, targetLabel)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    hist[\"loss\"].append(loss)\n",
    "    # print('Loss: {:.4f}'.format(loss))\n",
    "    \n",
    "    _, pred = model(test_data).max(dim=1)\n",
    "    correct = torch.eq(pred,test_data.y).sum().item()\n",
    "    acc = correct / test_graphData.num_nodes\n",
    "    hist[\"accuracy\"].append(acc)\n",
    "    # print('Accuracy: {:.4f}'.format(acc))\n",
    "    \n",
    "print('============================================')\n",
    "print('Loss: {:.4f}'.format(loss))\n",
    "print('Max. Accuracy: {:.4f}'.format(np.max(hist[\"accuracy\"])))\n",
    "print('Accuracy: {:.4f}'.format(np.mean(hist[\"accuracy\"][numEpoch-101:])))\n",
    "# print(torch.exp(out[0]))\n",
    "\n",
    "# Plotting Training Loss and Testing Accuracy ....\n",
    "print('============================================')\n",
    "print(' Plotting Training Loss and Testing Accuracy')\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.plot([e for e in range(numEpoch)], hist[\"loss\"], label=\"train_loss\")\n",
    "ax.plot([e for e in range(numEpoch)], hist[\"accuracy\"], label=\"test_acc\")\n",
    "plt.xlabel(\"Epoch\");\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "_, pred = model(test_data).max(dim=1)\n",
    "print(pred-test_data.y)\n",
    "correct = torch.eq(pred,test_data.y).sum().item()\n",
    "acc = correct / test_graphData.num_nodes\n",
    "sio.savemat('test_nodeClass.mat', {'nodeClass':np.array(pred)})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sl = torch.mean(torch.abs(model.conv1.weight.data),dim=-1)[0,train_graphData.num_node_features-1]\n",
    "s = torch.mean(torch.abs(model.conv1.weight.data),dim=-1)\n",
    "# slast = torch.mean(torch.abs(model.conv1.lin.weight.data),dim=0)[9]\n",
    "# sl = torch.mean(torch.abs(model.conv1.lin.weight.data),dim=0)/slast\n",
    "print('Average weight of first convolution layer::')\n",
    "print(s)\n",
    "# print(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print('name: ', name)\n",
    "    print(type(param))\n",
    "    print('param.shape: ', param.shape)\n",
    "    print('param.requires_grad: ', param.requires_grad)\n",
    "    print(param.data)\n",
    "    print('===========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
